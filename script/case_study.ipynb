{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../HiRES-2.0-BiModal/\")\n",
    "from EntityPairItem import EntityPairItem\n",
    "from data import RelationGraph\n",
    "from dgl import heterograph\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_result(path):\n",
    "    res = torch.load(path+\"test/result.pth\")\n",
    "    score = np.vstack(res[0])\n",
    "    true = np.array(res[2])\n",
    "    y_true = label_binarize(true, classes=[1,2,3])\n",
    "\n",
    "    precision = dict()\n",
    "    recall = dict()\n",
    "    threshold = dict()\n",
    "    average_precision = dict()\n",
    "    best_threshold = dict()\n",
    "    for i in range(score.shape[1]):\n",
    "        precision[i], recall[i], threshold[i] = precision_recall_curve(y_true[:, i], score[:, i])\n",
    "        average_precision[i] = average_precision_score(y_true[:, i], score[:, i])\n",
    "        f1 = 2*precision[i]*recall[i]/(precision[i] + recall[i] + 1e-10)\n",
    "        best_threshold[i] = threshold[i][np.argmax(f1)]\n",
    "    \n",
    "    print(\"annotated set:\")\n",
    "    pres = torch.load(path+\"pred/result.pth\")\n",
    "    pscore = np.vstack(pres[0])\n",
    "    ptrue = np.array(pres[2])\n",
    "    y_true = label_binarize(ptrue, classes=[1,2,3])\n",
    "    y_pred = np.array(pscore >= np.array(list(best_threshold.values())), dtype=np.int8)\n",
    "    for i in range(y_true.shape[1]):\n",
    "        print(i, accuracy_score(y_true[:,i], y_pred[:,i]))\n",
    "    print(accuracy_score(y_true.ravel(), y_pred.ravel()))\n",
    "    print(classification_report(y_true, y_pred, digits=3))\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated set:\n",
      "0 0.8376753507014028\n",
      "1 0.8717434869739479\n",
      "2 0.9238476953907816\n",
      "0.8777555110220441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.812     0.800     0.806       210\n",
      "           1      0.913     0.660     0.766       159\n",
      "           2      0.904     0.770     0.832       122\n",
      "\n",
      "   micro avg      0.862     0.747     0.800       491\n",
      "   macro avg      0.876     0.744     0.801       491\n",
      "weighted avg      0.867     0.747     0.800       491\n",
      " samples avg      0.735     0.735     0.735       491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "text_pred, text_true = loading_result(\"../HiRES-2.0-Text/result/proposal-bce-tucker/17/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated set:\n",
      "0 0.87374749498998\n",
      "1 0.8937875751503006\n",
      "2 0.9158316633266533\n",
      "0.8944555778223113\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.813     0.910     0.858       210\n",
      "           1      0.957     0.698     0.807       159\n",
      "           2      0.857     0.787     0.821       122\n",
      "\n",
      "   micro avg      0.860     0.811     0.834       491\n",
      "   macro avg      0.876     0.798     0.829       491\n",
      "weighted avg      0.870     0.811     0.832       491\n",
      " samples avg      0.785     0.798     0.789       491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "graph_pred, graph_true = loading_result(\"../HiRES-2.0-Graph/result/HAN_without_stys_TuckER/40/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated set:\n",
      "0 0.87374749498998\n",
      "1 0.8997995991983968\n",
      "2 0.9218436873747495\n",
      "0.8984635938543755\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.813     0.910     0.858       210\n",
      "           1      0.936     0.736     0.824       159\n",
      "           2      0.903     0.762     0.827       122\n",
      "\n",
      "   micro avg      0.866     0.817     0.841       491\n",
      "   macro avg      0.884     0.803     0.836       491\n",
      "weighted avg      0.875     0.817     0.839       491\n",
      " samples avg      0.796     0.804     0.798       491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "bimodal_graph_pred, bimodal_graph_true = loading_result(\n",
    "    \"../HiRES-2.0-BiModal/result/proposal-bce-tucker-biW-MinLogit-distill/10/graph-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotated set:\n",
      "0 0.843687374749499\n",
      "1 0.8917835671342685\n",
      "2 0.9238476953907816\n",
      "0.8864395457581831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.797     0.843     0.819       210\n",
      "           1      0.957     0.692     0.803       159\n",
      "           2      0.896     0.779     0.833       122\n",
      "\n",
      "   micro avg      0.862     0.778     0.818       491\n",
      "   macro avg      0.883     0.771     0.819       491\n",
      "weighted avg      0.873     0.778     0.818       491\n",
      " samples avg      0.766     0.766     0.766       491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/lyc/anaconda3/envs/py37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "bimodal_text_pred, bimodal_text_true = loading_result(\n",
    "    \"../HiRES-2.0-BiModal/result/proposal-bce-tucker-biW-MinLogit-distill/15/text-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (bimodal_text_true == bimodal_graph_true).all() and\\\n",
    "(text_true == graph_true).all() and\\\n",
    "(bimodal_text_true == graph_true).all()\n",
    "true = text_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/D2/EntityPairItems-disodiso-gold-500-token-term.jl\", \"rb\") as f:\n",
    "    test_data = joblib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "diso_graph = RelationGraph(\"../data/D2/\", 'DISO', 'DISO').transform()\n",
    "h = torch.from_numpy(np.load(\"../data/D2/embedtable.npy\")).float()\n",
    "h.requires_grad_(requires_grad=False)\n",
    "with open(\"../data/D2/cui2idx.pkl\", \"rb\") as f:\n",
    "    cui2idx = pickle.load(f)\n",
    "g = heterograph(diso_graph,num_nodes_dict={'DISO':h.size(0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_paths = [['DDx'], \n",
    "            ['May Be Caused By'], \n",
    "            ['May Cause'],\n",
    "            ['DDx','May Cause'],\n",
    "            ['DDx','May Be Caused By'],\n",
    "            ['May Cause','DDx'],\n",
    "            ['May Be Caused By','DDx'],\n",
    "            ['DDx','DDx'],\n",
    "            ['May Cause','May Cause'],\n",
    "            ['May Be Caused By','May Be Caused By']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_metas = [dgl.metapath_reachable_graph(g, meta_path) for meta_path in meta_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_meta_reachable(idx0, idx1):\n",
    "    metas = []\n",
    "    for i, g_meta in enumerate(g_metas):\n",
    "        head, tail = g_meta.edges(order=\"eid\")\n",
    "        indices_head = set(torch.where(head == idx0)[0].tolist())\n",
    "        indices_tail = set(torch.where(tail == idx1)[0].tolist())\n",
    "        if len(indices_head.intersection(indices_tail)) != 0:\n",
    "            metas.append(meta_paths[i])\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree(idx):\n",
    "    return [g.out_degrees(idx, etype) for etype in g.etypes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSB analysis between with/without Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 354 85 14\n"
     ]
    }
   ],
   "source": [
    "G = []\n",
    "GS = []\n",
    "BS = []\n",
    "B = []\n",
    "for idx, (text, bi_text, true_label) in enumerate(zip(text_pred, bimodal_text_pred, true)):\n",
    "    if (text == bi_text).all():\n",
    "        if (text == true_label).all():\n",
    "            GS.append(idx)\n",
    "        else:\n",
    "            BS.append(idx)\n",
    "    if not (text == true_label).all() and (bi_text == true_label).all():\n",
    "        G.append(idx)\n",
    "    if (text == true_label).all() and not (bi_text == true_label).all():\n",
    "        B.append(idx)\n",
    "print(len(G), len(GS), len(BS), len(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data = {\n",
    "    \"cui1\": [],\n",
    "    \"cui2\": [],\n",
    "    \"sentences\": [],\n",
    "    \"sent_num\": [],\n",
    "    \"cui1_degree\": [],\n",
    "    \"cui2_degree\": [],\n",
    "    \"meta_path_reachable\": [],\n",
    "    \"label\": [],\n",
    "    \"case_type\": []}\n",
    "for i in G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"REMOD_G(text)\")\n",
    "for i in B:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"REMOD_B(text)\")\n",
    "df_case_data_text = pd.DataFrame(case_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSB analysis between with/without Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 375 80 11\n"
     ]
    }
   ],
   "source": [
    "G = []\n",
    "GS = []\n",
    "BS = []\n",
    "B = []\n",
    "for idx, (graph, bi_graph, true_label) in enumerate(zip(graph_pred, bimodal_graph_pred, true)):\n",
    "    if (graph == bi_graph).all():\n",
    "        if (graph == true_label).all():\n",
    "            GS.append(idx)\n",
    "        else:\n",
    "            BS.append(idx)\n",
    "    if not (graph == true_label).all() and (bi_graph == true_label).all():\n",
    "        G.append(idx)\n",
    "    if (graph == true_label).all() and not (bi_graph == true_label).all():\n",
    "        B.append(idx)\n",
    "print(len(G), len(GS), len(BS), len(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data = {\n",
    "    \"cui1\": [],\n",
    "    \"cui2\": [],\n",
    "    \"sentences\": [],\n",
    "    \"sent_num\": [],\n",
    "    \"cui1_degree\": [],\n",
    "    \"cui2_degree\": [],\n",
    "    \"meta_path_reachable\": [],\n",
    "    \"label\": [],\n",
    "    \"case_type\": []}\n",
    "for i in G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"REMOD_G(graph)\")\n",
    "for i in B:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"REMOD_B(graph)\")\n",
    "df_case_data_graph = pd.DataFrame(case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_case_data_text, df_case_data_graph]).to_csv(\"REMOD_case_study_compare_with_single_modality.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 348 49 46\n"
     ]
    }
   ],
   "source": [
    "text_G = []\n",
    "GS = []\n",
    "BS = []\n",
    "graph_G = []\n",
    "for idx, (bi_text, bi_graph, true_label) in enumerate(zip(bimodal_text_pred, bimodal_graph_pred, true)):\n",
    "    if (bi_text == bi_graph).all():\n",
    "        if (bi_text == true_label).all():\n",
    "            GS.append(idx)\n",
    "        else:\n",
    "            BS.append(idx)\n",
    "    if not (bi_text == true_label).all() and (bi_graph == true_label).all():\n",
    "        graph_G.append(idx)\n",
    "    if (bi_text == true_label).all() and not (bi_graph == true_label).all():\n",
    "        text_G.append(idx)\n",
    "print(len(text_G), len(GS), len(BS), len(graph_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data = {\n",
    "    \"cui1\": [],\n",
    "    \"cui2\": [],\n",
    "    \"sentences\": [],\n",
    "    \"sent_num\": [],\n",
    "    \"cui1_degree\": [],\n",
    "    \"cui2_degree\": [],\n",
    "    \"meta_path_reachable\": [],\n",
    "    \"label\": [],\n",
    "    \"case_type\": []}\n",
    "for i in text_G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"text_G(REMOD)\")\n",
    "for i in graph_G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"graph_G(REMOD)\")\n",
    "df_case_data_remod = pd.DataFrame(case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 337 59 49\n"
     ]
    }
   ],
   "source": [
    "text_G = []\n",
    "GS = []\n",
    "BS = []\n",
    "graph_G = []\n",
    "for idx, (text, graph, true_label) in enumerate(zip(text_pred, graph_pred, true)):\n",
    "    if (text == graph).all():\n",
    "        if (text == true_label).all():\n",
    "            GS.append(idx)\n",
    "        else:\n",
    "            BS.append(idx)\n",
    "    if not (text == true_label).all() and (graph == true_label).all():\n",
    "        graph_G.append(idx)\n",
    "    if (text == true_label).all() and not (graph == true_label).all():\n",
    "        text_G.append(idx)\n",
    "print(len(text_G), len(GS), len(BS), len(graph_G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data = {\n",
    "    \"cui1\": [],\n",
    "    \"cui2\": [],\n",
    "    \"sentences\": [],\n",
    "    \"sent_num\": [],\n",
    "    \"cui1_degree\": [],\n",
    "    \"cui2_degree\": [],\n",
    "    \"meta_path_reachable\": [],\n",
    "    \"label\": [],\n",
    "    \"case_type\": []}\n",
    "for i in text_G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"text_G(Baseline)\")\n",
    "for i in graph_G:\n",
    "    cui1 = test_data[i].cui1\n",
    "    cui2 = test_data[i].cui2\n",
    "    case_data[\"cui1\"].append(test_data[i].cui1)\n",
    "    case_data[\"cui2\"].append(test_data[i].cui2)\n",
    "    case_data[\"sentences\"].append(test_data[i].sentences)\n",
    "    case_data[\"sent_num\"].append(len(test_data[i].sentences))\n",
    "    case_data[\"cui1_degree\"].append(json.dumps(get_degree(cui2idx[cui1])))\n",
    "    case_data[\"cui2_degree\"].append(json.dumps(get_degree(cui2idx[cui2])))\n",
    "    case_data[\"meta_path_reachable\"].append(json.dumps(check_meta_reachable(cui2idx[cui1], cui2idx[cui2])))\n",
    "    case_data[\"label\"].append(int(test_data[i].label))\n",
    "    case_data[\"case_type\"].append(\"graph_G(Baseline)\")\n",
    "df_case_data_baseline = pd.DataFrame(case_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_case_data_remod, df_case_data_baseline]).to_csv(\"REMOD_case_study_compare_between_text_and_graph.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
